{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "da0236e4b36ce514c1fec3fd72f236d1fa259131",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2851,
     "status": "ok",
     "timestamp": 1590183583979,
     "user": {
      "displayName": "Harry Li",
      "photoUrl": "",
      "userId": "15703030582002833452"
     },
     "user_tz": -60
    },
    "id": "8STOjdLMHdxR",
    "outputId": "771ff2b6-ebe5-4a7d-e61b-7a4a87da1659",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plaidml.keras\n",
    "plaidml.keras.install_backend()\n",
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"plaidml.keras.backend\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing useful libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, GRU, Bidirectional, Conv1D, Flatten, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import optimizers\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 404954,
     "status": "ok",
     "timestamp": 1590180738140,
     "user": {
      "displayName": "Harry Li",
      "photoUrl": "",
      "userId": "15703030582002833452"
     },
     "user_tz": -60
    },
    "id": "4Kvu13YOIJvd",
    "outputId": "79f171a8-e981-4fba-d92f-dd5d9bfd941e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import files\n",
    "# uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4PuEzi8NIfJv"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/num_data.csv')\n",
    "\n",
    "# import io\n",
    "# df = pd.read_csv(io.BytesIO(uploaded['num_data.csv']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_rmse(test,predicted):\n",
    "    rmse = math.sqrt(mean_squared_error(test, predicted))\n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kWPg4bN2HdxV"
   },
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c_vM_97JHdxV",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/num_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZdU5H-QeHdxX"
   },
   "outputs": [],
   "source": [
    "POLLUTION = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ODJBLZODHdxa"
   },
   "outputs": [],
   "source": [
    "WEATHER = ['PM2.5', 'TEMP', 'PRES', 'DEWP', 'RAIN', 'wd', 'WSPM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h_lZlRU4Hdxc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1152,
     "status": "ok",
     "timestamp": 1590180973115,
     "user": {
      "displayName": "Harry Li",
      "photoUrl": "",
      "userId": "15703030582002833452"
     },
     "user_tz": -60
    },
    "id": "Qh2SvmxSHdxe",
    "outputId": "ebfe8e7a-35d7-4ef1-9ac7-a473427bdf35"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420768, 16)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HBAjRBmtHdxj",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_size = dataset.shape[0]\n",
    "train_size=int(data_size * 0.6)\n",
    "test_size = int(data_size * 0.2)\n",
    "valid_size = data_size - train_size - test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "fb4c9db6d8a5bcf20ffad41747cfa5b6215ba220",
    "colab": {},
    "colab_type": "code",
    "id": "w8k14fbcHdxl",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_set = dataset[:train_size].iloc[:,4:16].values\n",
    "valid_set = dataset[train_size:train_size+valid_size].iloc[:,4:16].values\n",
    "test_set = dataset[data_size-test_size:].iloc[:,4:16].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1134,
     "status": "ok",
     "timestamp": 1590180973116,
     "user": {
      "displayName": "Harry Li",
      "photoUrl": "",
      "userId": "15703030582002833452"
     },
     "user_tz": -60
    },
    "id": "y-4QTCotHdxm",
    "outputId": "74a47baf-9638-462c-9b96-0b30fe1bef56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(420768, 1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = dataset.iloc[:,0].values\n",
    "y = y.reshape(-1,1)\n",
    "n_feature = training_set.shape[1]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "bcc9c36165fc07d258bd5ea87874d2da17fa4a4d",
    "colab": {},
    "colab_type": "code",
    "id": "bkq_8JaAHdxp",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scaling the dataset\n",
    "sc = MinMaxScaler(feature_range=(0,1))\n",
    "training_set_scaled = sc.fit_transform(training_set)\n",
    "valid_set_scaled = sc.fit_transform(valid_set)\n",
    "test_set_scaled = sc.fit_transform(test_set)\n",
    "\n",
    "sc_y = MinMaxScaler(feature_range=(0,1))\n",
    "y_scaled = sc_y.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ui_miaiTHdxr"
   },
   "outputs": [],
   "source": [
    "# convert dataset into sequences, where n_steps_in is the input sequence lengith, \n",
    "# and n_steps_out is the output sequence length \n",
    "def convert_to_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X_, y_ = [], []\n",
    "    for i in range(len(sequences)):\n",
    "        tail_x = i + n_steps_in\n",
    "        out_tail_x = tail_x + n_steps_out-1\n",
    "        if out_tail_x > len(sequences):\n",
    "            break\n",
    "        seq_x, seq_y = sequences[i:tail_x, :], sequences[tail_x-1:out_tail_x, 0]\n",
    "        X_.append(seq_x)\n",
    "        y_.append(seq_y)\n",
    "    return np.array(X_), np.array(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pG3YEhxlHdxt"
   },
   "outputs": [],
   "source": [
    "n_steps_in = 12\n",
    "n_steps_out = 12\n",
    "X_train, y_train = convert_to_sequences(training_set_scaled, n_steps_in, n_steps_out)\n",
    "X_valid, y_valid = convert_to_sequences(valid_set_scaled, n_steps_in, n_steps_out)\n",
    "X_test, y_test = convert_to_sequences(test_set_scaled, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGEaAkVuHdxv"
   },
   "source": [
    "## Grid Search Control \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F9bL6T_tHdxv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Opening device \"metal_intel(r)_iris(tm)_graphics_6100.0\"\n"
     ]
    }
   ],
   "source": [
    "n_activation = ['tanh', 'sigmoid', 'relu']\n",
    "act = n_activation[1]\n",
    "\n",
    "n_learn_rate = [0.01, 0.001, 0.0001]\n",
    "lr = n_learn_rate[0]\n",
    "\n",
    "n_optimizers = [optimizers.Adam(lr=lr), optimizers.RMSprop(lr=lr), optimizers.SGD(lr=lr)]\n",
    "opt = n_optimizers[0]\n",
    "\n",
    "n_epoches = [50]\n",
    "epoch = n_epoches[0]\n",
    "\n",
    "n_batch_size = [1024, 2048, 5096]\n",
    "batch = n_batch_size[-1]\n",
    "\n",
    "n_of_neurons = [10, 50, 200]\n",
    "neuron = n_of_neurons[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjuDlHhoHdxx"
   },
   "outputs": [],
   "source": [
    "rmse_df = pd.DataFrame(columns=['Model', 'train_rmse', 'valid_rmse', 'test_rmse', 'train_time', 'epoch', \n",
    "                               'batch', 'neuron'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "id": "b1mxBYLJHdxz",
    "outputId": "c1c7db63-006f-46e8-ddba-32da61c74e21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start for LSTM_LSTM\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2198 of 2743 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252416/252438 [============================>.] - ETA: 0s - loss: 0.0036"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 2102 of 2743 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252438/252438 [==============================] - 215s 851us/step - loss: 0.0036\n",
      "Epoch 2/50\n",
      "252438/252438 [==============================] - 189s 748us/step - loss: 0.0024\n",
      "Epoch 3/50\n",
      "252438/252438 [==============================] - 195s 773us/step - loss: 0.0023\n",
      "Epoch 4/50\n",
      "252438/252438 [==============================] - 189s 748us/step - loss: 0.0022\n",
      "Epoch 5/50\n",
      "252438/252438 [==============================] - 171s 678us/step - loss: 0.0022\n",
      "Epoch 6/50\n",
      "252438/252438 [==============================] - 194s 767us/step - loss: 0.0021\n",
      "Epoch 7/50\n",
      "252438/252438 [==============================] - 191s 758us/step - loss: 0.0021\n",
      "Epoch 8/50\n",
      "252438/252438 [==============================] - 191s 757us/step - loss: 0.0021\n",
      "Epoch 9/50\n",
      "252438/252438 [==============================] - 191s 756us/step - loss: 0.0020\n",
      "Epoch 10/50\n",
      "252438/252438 [==============================] - 170s 672us/step - loss: 0.0020\n",
      "Epoch 11/50\n",
      "252438/252438 [==============================] - 191s 756us/step - loss: 0.0020\n",
      "Epoch 12/50\n",
      "252438/252438 [==============================] - 191s 756us/step - loss: 0.0020\n",
      "Epoch 13/50\n",
      "252438/252438 [==============================] - 191s 758us/step - loss: 0.0019\n",
      "Epoch 14/50\n",
      "252438/252438 [==============================] - 191s 758us/step - loss: 0.0019\n",
      "Epoch 15/50\n",
      "252438/252438 [==============================] - 169s 670us/step - loss: 0.0019\n",
      "Epoch 16/50\n",
      "252438/252438 [==============================] - 191s 758us/step - loss: 0.0019\n",
      "Epoch 17/50\n",
      "252438/252438 [==============================] - 191s 757us/step - loss: 0.0018\n",
      "Epoch 18/50\n",
      "252438/252438 [==============================] - 191s 757us/step - loss: 0.0018\n",
      "Epoch 19/50\n",
      "252438/252438 [==============================] - 191s 758us/step - loss: 0.0018\n",
      "Epoch 20/50\n",
      "252438/252438 [==============================] - 169s 668us/step - loss: 0.0018\n",
      "Epoch 21/50\n",
      "252438/252438 [==============================] - 191s 757us/step - loss: 0.0018\n",
      "Epoch 22/50\n",
      "252438/252438 [==============================] - 191s 757us/step - loss: 0.0018\n",
      "Epoch 23/50\n",
      "252438/252438 [==============================] - 191s 758us/step - loss: 0.0017\n",
      "Epoch 24/50\n",
      "252438/252438 [==============================] - 192s 759us/step - loss: 0.0017\n",
      "Epoch 25/50\n",
      "252438/252438 [==============================] - 172s 680us/step - loss: 0.0017\n",
      "Epoch 26/50\n",
      "252438/252438 [==============================] - 188s 746us/step - loss: 0.0017\n",
      "Epoch 27/50\n",
      "252438/252438 [==============================] - 191s 757us/step - loss: 0.0017\n",
      "Epoch 28/50\n",
      "252438/252438 [==============================] - 191s 758us/step - loss: 0.0017\n",
      "Epoch 29/50\n",
      "252438/252438 [==============================] - 192s 759us/step - loss: 0.0017\n",
      "Epoch 30/50\n",
      "252438/252438 [==============================] - 172s 681us/step - loss: 0.0016\n",
      "Epoch 31/50\n",
      "252438/252438 [==============================] - 222s 880us/step - loss: 0.0016\n",
      "Epoch 32/50\n",
      "252438/252438 [==============================] - 228s 902us/step - loss: 0.0016\n",
      "Epoch 33/50\n",
      "252438/252438 [==============================] - 197s 782us/step - loss: 0.0016\n",
      "Epoch 34/50\n",
      "252438/252438 [==============================] - 205s 811us/step - loss: 0.0016\n",
      "Epoch 35/50\n",
      "252438/252438 [==============================] - 228s 903us/step - loss: 0.0016\n",
      "Epoch 36/50\n",
      "252438/252438 [==============================] - 214s 848us/step - loss: 0.0016\n",
      "Epoch 37/50\n",
      "252438/252438 [==============================] - 186s 738us/step - loss: 0.0016\n",
      "Epoch 38/50\n",
      "252438/252438 [==============================] - 227s 900us/step - loss: 0.0016\n",
      "Epoch 39/50\n",
      "252438/252438 [==============================] - 227s 900us/step - loss: 0.0015\n",
      "Epoch 40/50\n",
      "252438/252438 [==============================] - 173s 687us/step - loss: 0.0016\n",
      "Epoch 41/50\n",
      "252438/252438 [==============================] - 227s 901us/step - loss: 0.0015\n",
      "Epoch 42/50\n",
      "252438/252438 [==============================] - 228s 902us/step - loss: 0.0015\n",
      "Epoch 43/50\n",
      "252438/252438 [==============================] - 191s 756us/step - loss: 0.0015s - loss:\n",
      "Epoch 44/50\n",
      "252438/252438 [==============================] - 211s 837us/step - loss: 0.0015\n",
      "Epoch 45/50\n",
      "252438/252438 [==============================] - 227s 900us/step - loss: 0.0015\n",
      "Epoch 46/50\n",
      "252438/252438 [==============================] - 209s 826us/step - loss: 0.0015\n",
      "Epoch 47/50\n",
      "252438/252438 [==============================] - 193s 766us/step - loss: 0.0015\n",
      "Epoch 48/50\n",
      "252438/252438 [==============================] - 227s 900us/step - loss: 0.0015\n",
      "Epoch 49/50\n",
      "252438/252438 [==============================] - 226s 896us/step - loss: 0.0015\n",
      "Epoch 50/50\n",
      "252438/252438 [==============================] - 200s 792us/step - loss: 0.0015\n",
      "results for training set\n",
      "results for valid set\n",
      "results for test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Harry/Documents/apple/env/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start for LSTM_LSTM\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:plaidml:Analyzing Ops: 1864 of 2747 operations complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "252438/252438 [==============================] - 331s 1ms/step - loss: 0.0323\n",
      "Epoch 2/50\n",
      "252438/252438 [==============================] - 228s 905us/step - loss: 0.0275\n",
      "Epoch 3/50\n",
      "252438/252438 [==============================] - 323s 1ms/step - loss: 0.0268\n",
      "Epoch 4/50\n",
      "252438/252438 [==============================] - 220s 872us/step - loss: 0.0265s\n",
      "Epoch 5/50\n",
      "252438/252438 [==============================] - 328s 1ms/step - loss: 0.0262\n",
      "Epoch 6/50\n",
      "252438/252438 [==============================] - 215s 851us/step - loss: 0.0261\n",
      "Epoch 7/50\n",
      "252438/252438 [==============================] - 329s 1ms/step - loss: 0.0258\n",
      "Epoch 8/50\n",
      "252438/252438 [==============================] - 212s 840us/step - loss: 0.0257\n",
      "Epoch 9/50\n",
      "252438/252438 [==============================] - 377s 1ms/step - loss: 0.0256\n",
      "Epoch 10/50\n",
      "252438/252438 [==============================] - 315s 1ms/step - loss: 0.0255\n",
      "Epoch 11/50\n",
      "252438/252438 [==============================] - 229s 908us/step - loss: 0.0253\n",
      "Epoch 12/50\n",
      "252438/252438 [==============================] - 374s 1ms/step - loss: 0.0253\n",
      "Epoch 13/50\n",
      "252438/252438 [==============================] - 330s 1ms/step - loss: 0.0252\n",
      "Epoch 14/50\n",
      "252438/252438 [==============================] - 219s 868us/step - loss: 0.0251\n",
      "Epoch 15/50\n",
      "252438/252438 [==============================] - 368s 1ms/step - loss: 0.0250\n",
      "Epoch 16/50\n",
      "252438/252438 [==============================] - 339s 1ms/step - loss: 0.0250\n",
      "Epoch 17/50\n",
      "252438/252438 [==============================] - 196s 778us/step - loss: 0.0248\n",
      "Epoch 18/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0248\n",
      "Epoch 19/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0247\n",
      "Epoch 20/50\n",
      "252438/252438 [==============================] - 136s 539us/step - loss: 0.0246\n",
      "Epoch 21/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0246\n",
      "Epoch 22/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0245\n",
      "Epoch 23/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0244\n",
      "Epoch 24/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0243\n",
      "Epoch 25/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0243\n",
      "Epoch 26/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0242\n",
      "Epoch 27/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0242\n",
      "Epoch 28/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0241\n",
      "Epoch 29/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0241\n",
      "Epoch 30/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0240\n",
      "Epoch 31/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0240\n",
      "Epoch 32/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0239\n",
      "Epoch 33/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0238\n",
      "Epoch 34/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0238\n",
      "Epoch 35/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0238\n",
      "Epoch 36/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0237\n",
      "Epoch 37/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0237\n",
      "Epoch 38/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0236\n",
      "Epoch 39/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0236\n",
      "Epoch 40/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0236\n",
      "Epoch 41/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0235\n",
      "Epoch 42/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0235\n",
      "Epoch 43/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0234\n",
      "Epoch 44/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0234\n",
      "Epoch 45/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0233\n",
      "Epoch 46/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0234\n",
      "Epoch 47/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0233\n",
      "Epoch 48/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0233\n",
      "Epoch 49/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0232\n",
      "Epoch 50/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0232\n",
      "results for training set\n",
      "results for valid set\n",
      "results for test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Harry/Documents/apple/env/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start for LSTM_LSTM\n",
      "Epoch 1/50\n",
      "252438/252438 [==============================] - 136s 540us/step - loss: 0.0048\n",
      "Epoch 2/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0027\n",
      "Epoch 3/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0025\n",
      "Epoch 4/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0024\n",
      "Epoch 5/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0024\n",
      "Epoch 6/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0024\n",
      "Epoch 7/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0023\n",
      "Epoch 8/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0023\n",
      "Epoch 9/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0023\n",
      "Epoch 10/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0023\n",
      "Epoch 11/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0023\n",
      "Epoch 12/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0022\n",
      "Epoch 13/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0022\n",
      "Epoch 14/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0022\n",
      "Epoch 15/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0022\n",
      "Epoch 16/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0022\n",
      "Epoch 17/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0022\n",
      "Epoch 18/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0022\n",
      "Epoch 19/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0022\n",
      "Epoch 20/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0022\n",
      "Epoch 21/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0022\n",
      "Epoch 22/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0022\n",
      "Epoch 23/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0021\n",
      "Epoch 24/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0021\n",
      "Epoch 25/50\n",
      "252438/252438 [==============================] - 136s 539us/step - loss: 0.0021\n",
      "Epoch 26/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0021\n",
      "Epoch 27/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0021\n",
      "Epoch 28/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0021\n",
      "Epoch 29/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0021\n",
      "Epoch 30/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0021\n",
      "Epoch 31/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0021\n",
      "Epoch 32/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0021\n",
      "Epoch 33/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0021\n",
      "Epoch 34/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0021\n",
      "Epoch 35/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0021\n",
      "Epoch 36/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0021\n",
      "Epoch 37/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "Epoch 38/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0020\n",
      "Epoch 39/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "Epoch 40/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "Epoch 41/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "Epoch 42/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0020\n",
      "Epoch 43/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "Epoch 44/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "Epoch 45/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "Epoch 46/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "Epoch 47/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0020\n",
      "Epoch 48/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0020\n",
      "Epoch 49/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "Epoch 50/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0020\n",
      "results for training set\n",
      "results for valid set\n",
      "results for test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Harry/Documents/apple/env/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start for LSTM_LSTM\n",
      "Epoch 1/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0373\n",
      "Epoch 2/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0290\n",
      "Epoch 3/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0283\n",
      "Epoch 4/50\n",
      "252438/252438 [==============================] - 136s 539us/step - loss: 0.0279\n",
      "Epoch 5/50\n",
      "252438/252438 [==============================] - 136s 539us/step - loss: 0.0277\n",
      "Epoch 6/50\n",
      "252438/252438 [==============================] - 136s 539us/step - loss: 0.0276\n",
      "Epoch 7/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0275\n",
      "Epoch 8/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0273\n",
      "Epoch 9/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0271\n",
      "Epoch 10/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0270\n",
      "Epoch 11/50\n",
      "252438/252438 [==============================] - 136s 539us/step - loss: 0.0269\n",
      "Epoch 12/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0268\n",
      "Epoch 13/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0267\n",
      "Epoch 14/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0267\n",
      "Epoch 15/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0266\n",
      "Epoch 16/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0266\n",
      "Epoch 17/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0265\n",
      "Epoch 18/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0265\n",
      "Epoch 19/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0264\n",
      "Epoch 20/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0264\n",
      "Epoch 21/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0263\n",
      "Epoch 22/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0263\n",
      "Epoch 23/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0263\n",
      "Epoch 24/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0262\n",
      "Epoch 25/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0262\n",
      "Epoch 26/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0261\n",
      "Epoch 27/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0261\n",
      "Epoch 28/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0261\n",
      "Epoch 29/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0261\n",
      "Epoch 30/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0260\n",
      "Epoch 31/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0260\n",
      "Epoch 32/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0260\n",
      "Epoch 33/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0259\n",
      "Epoch 34/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0259\n",
      "Epoch 35/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0259\n",
      "Epoch 36/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0258\n",
      "Epoch 37/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0258\n",
      "Epoch 38/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0258\n",
      "Epoch 39/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0257\n",
      "Epoch 40/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0257\n",
      "Epoch 41/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0257\n",
      "Epoch 42/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0257\n",
      "Epoch 43/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0257\n",
      "Epoch 44/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0256\n",
      "Epoch 45/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0256\n",
      "Epoch 46/50\n",
      "252438/252438 [==============================] - 136s 539us/step - loss: 0.0256\n",
      "Epoch 47/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0256\n",
      "Epoch 48/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0255\n",
      "Epoch 49/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0255\n",
      "Epoch 50/50\n",
      "252438/252438 [==============================] - 137s 542us/step - loss: 0.0255\n",
      "results for training set\n",
      "results for valid set\n",
      "results for test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Harry/Documents/apple/env/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start for LSTM_LSTM\n",
      "Epoch 1/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0217\n",
      "Epoch 2/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0062\n",
      "Epoch 3/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0042\n",
      "Epoch 4/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0033\n",
      "Epoch 5/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0031\n",
      "Epoch 6/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0029\n",
      "Epoch 7/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0028\n",
      "Epoch 8/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0027\n",
      "Epoch 9/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0027\n",
      "Epoch 10/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0026\n",
      "Epoch 11/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0026\n",
      "Epoch 12/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0026\n",
      "Epoch 13/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0025\n",
      "Epoch 14/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0025\n",
      "Epoch 15/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0025\n",
      "Epoch 16/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0025\n",
      "Epoch 17/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0025\n",
      "Epoch 18/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0024\n",
      "Epoch 19/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0024\n",
      "Epoch 20/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0024\n",
      "Epoch 21/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0024\n",
      "Epoch 22/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0024\n",
      "Epoch 23/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0024\n",
      "Epoch 24/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0024\n",
      "Epoch 25/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0024\n",
      "Epoch 26/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0024\n",
      "Epoch 27/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0024\n",
      "Epoch 28/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0024\n",
      "Epoch 29/50\n",
      "252438/252438 [==============================] - 136s 537us/step - loss: 0.0024\n",
      "Epoch 30/50\n",
      "252438/252438 [==============================] - 135s 536us/step - loss: 0.0023\n",
      "Epoch 31/50\n",
      "252438/252438 [==============================] - 136s 538us/step - loss: 0.0023\n",
      "Epoch 32/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0023\n",
      "Epoch 33/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0023\n",
      "Epoch 34/50\n",
      "252438/252438 [==============================] - 135s 537us/step - loss: 0.0023\n",
      "Epoch 35/50\n",
      "252438/252438 [==============================] - 138s 548us/step - loss: 0.0023\n",
      "Epoch 36/50\n",
      "252438/252438 [==============================] - 131s 520us/step - loss: 0.0023\n",
      "Epoch 37/50\n",
      "252438/252438 [==============================] - 130s 515us/step - loss: 0.0023\n",
      "Epoch 38/50\n",
      "252438/252438 [==============================] - 130s 515us/step - loss: 0.0023\n",
      "Epoch 39/50\n",
      "252438/252438 [==============================] - 130s 516us/step - loss: 0.0023\n",
      "Epoch 40/50\n",
      "252438/252438 [==============================] - 130s 516us/step - loss: 0.0023\n",
      "Epoch 41/50\n",
      "252438/252438 [==============================] - 130s 515us/step - loss: 0.0023\n",
      "Epoch 42/50\n",
      "252438/252438 [==============================] - 130s 516us/step - loss: 0.0023\n",
      "Epoch 43/50\n",
      "252438/252438 [==============================] - 130s 515us/step - loss: 0.0023\n",
      "Epoch 44/50\n",
      "252438/252438 [==============================] - 130s 516us/step - loss: 0.0023\n",
      "Epoch 45/50\n",
      "252438/252438 [==============================] - 130s 516us/step - loss: 0.0023\n",
      "Epoch 46/50\n",
      "252438/252438 [==============================] - 130s 515us/step - loss: 0.0023\n",
      "Epoch 47/50\n",
      "252438/252438 [==============================] - 130s 515us/step - loss: 0.0023\n",
      "Epoch 48/50\n",
      "252438/252438 [==============================] - 130s 515us/step - loss: 0.0023\n",
      "Epoch 49/50\n",
      "252438/252438 [==============================] - 130s 515us/step - loss: 0.0023\n",
      "Epoch 50/50\n",
      "252438/252438 [==============================] - 130s 515us/step - loss: 0.0023\n",
      "results for training set\n",
      "results for valid set\n",
      "results for test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Harry/Documents/apple/env/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training start for LSTM_LSTM\n",
      "Epoch 1/50\n",
      "252438/252438 [==============================] - 127s 504us/step - loss: 0.0537\n",
      "Epoch 2/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0370\n",
      "Epoch 3/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0350\n",
      "Epoch 4/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0338\n",
      "Epoch 5/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0329\n",
      "Epoch 6/50\n",
      "252438/252438 [==============================] - 127s 503us/step - loss: 0.0321\n",
      "Epoch 7/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0314\n",
      "Epoch 8/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0308\n",
      "Epoch 9/50\n",
      "252438/252438 [==============================] - 127s 503us/step - loss: 0.0303\n",
      "Epoch 10/50\n",
      "252438/252438 [==============================] - 127s 503us/step - loss: 0.0300\n",
      "Epoch 11/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0297\n",
      "Epoch 12/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0294\n",
      "Epoch 13/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0292\n",
      "Epoch 14/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0291\n",
      "Epoch 15/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0289\n",
      "Epoch 16/50\n",
      "252438/252438 [==============================] - 127s 503us/step - loss: 0.0288\n",
      "Epoch 17/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0287\n",
      "Epoch 18/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0286\n",
      "Epoch 19/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0286\n",
      "Epoch 20/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0285\n",
      "Epoch 21/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0284\n",
      "Epoch 22/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0284\n",
      "Epoch 23/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0283\n",
      "Epoch 24/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0283\n",
      "Epoch 25/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0282\n",
      "Epoch 26/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0282\n",
      "Epoch 27/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0281\n",
      "Epoch 28/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0281\n",
      "Epoch 29/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0281\n",
      "Epoch 30/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0280\n",
      "Epoch 31/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0280\n",
      "Epoch 32/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0280\n",
      "Epoch 33/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0279\n",
      "Epoch 34/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0279\n",
      "Epoch 35/50\n",
      "252438/252438 [==============================] - 127s 505us/step - loss: 0.0279\n",
      "Epoch 36/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0279\n",
      "Epoch 37/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0278\n",
      "Epoch 38/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0278\n",
      "Epoch 39/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0278\n",
      "Epoch 40/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0278\n",
      "Epoch 41/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0278\n",
      "Epoch 42/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0277\n",
      "Epoch 43/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0277\n",
      "Epoch 44/50\n",
      "252438/252438 [==============================] - 127s 502us/step - loss: 0.0277\n",
      "Epoch 45/50\n",
      "252438/252438 [==============================] - 126s 501us/step - loss: 0.0277\n",
      "Epoch 46/50\n",
      "252438/252438 [==============================] - 126s 501us/step - loss: 0.0277\n",
      "Epoch 47/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0277\n",
      "Epoch 48/50\n",
      "252438/252438 [==============================] - 126s 501us/step - loss: 0.0276\n",
      "Epoch 49/50\n",
      "252438/252438 [==============================] - 127s 501us/step - loss: 0.0276\n",
      "Epoch 50/50\n",
      "252438/252438 [==============================] - 126s 501us/step - loss: 0.0276\n",
      "results for training set\n",
      "results for valid set\n",
      "results for test set\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Harry/Documents/apple/env/lib/python3.7/site-packages/ipykernel_launcher.py:38: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in [256]:\n",
    "    for lr in n_learn_rate:\n",
    "        n_optimizers = [optimizers.Adam(lr=lr)]\n",
    "        for opt in n_optimizers: \n",
    "            for loss_function in ['mean_squared_error', 'mean_absolute_error']:\n",
    "                LSTM_LSTM_reg = Sequential()\n",
    "                LSTM_LSTM_reg.add(LSTM(units=neuron, return_sequences=True, input_shape=(X_train.shape[1],n_feature), activation=act))\n",
    "                LSTM_LSTM_reg.add(LSTM(units=neuron, activation=act))\n",
    "                LSTM_LSTM_reg.add(Dense(units=n_steps_out))\n",
    "\n",
    "                LSTM_LSTM_reg.compile(optimizer=opt,loss=loss_function)\n",
    "\n",
    "\n",
    "                regressor = LSTM_LSTM_reg\n",
    "                model = 'LSTM_LSTM'\n",
    "\n",
    "                print('training start for', model)    \n",
    "                start = time.process_time()\n",
    "                regressor.fit(X_train,y_train,epochs=epoch,batch_size=batch)\n",
    "                train_time = round(time.process_time() - start, 2)\n",
    "\n",
    "                print('results for training set')\n",
    "                y_train_pred = regressor.predict(X_train)\n",
    "                train_rmse = return_rmse(y_train,y_train_pred)\n",
    "\n",
    "                print('results for valid set')\n",
    "                y_valid_pred = regressor.predict(X_valid)\n",
    "                valid_rmse = return_rmse(y_valid,y_valid_pred)\n",
    "\n",
    "                print('results for test set')\n",
    "                y_test_pred = regressor.predict(X_test)\n",
    "                test_rmse = return_rmse(y_test,y_test_pred)\n",
    "\n",
    "\n",
    "                one_df = pd.DataFrame([[model, train_rmse, valid_rmse, test_rmse, train_time, batch, lr, loss_function ]],\n",
    "                              columns=['Model', 'train_rmse', 'valid_rmse', 'test_rmse', 'train_time', 'Batch Size', \n",
    "                                   'Learning Rate', 'Loss Function'])\n",
    "                rmse_df = pd.concat([rmse_df, one_df])\n",
    "\n",
    "# save the rmse results \n",
    "rmse_df.to_csv('../2LSTM_grid_search_part3.csv')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Is9FXlIHdx1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4DBux1cMHdx3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FMXNGD-VHdx5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Colab CBGRU multistep_GridSearch_v1.ipynb",
   "provenance": [
    {
     "file_id": "1CUZBQ3loiSXzBGyoKZXNqXAWnU9Qavrm",
     "timestamp": 1590181336394
    }
   ]
  },
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
